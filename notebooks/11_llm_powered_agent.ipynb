{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95f0f9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "🧠 Welcome to LLM-Powered Agents!\n",
      "============================================================\n",
      "🔧 Environment Setup Complete!\n",
      "📍 Project directory: c:\\Users\\Sarthak\\OneDrive\\Documents\\Desktop\\Python projects\\genai-workshop\n",
      "📅 Session started: 2025-07-21 13:32:32\n",
      "\n",
      "🎯 Today's Journey:\n",
      "   🔄 1. Compare Rule-Based vs LLM Approaches\n",
      "   🧠 2. Build LLM Integration System\n",
      "   🛠️ 3. Create Intelligent Tool Selection\n",
      "   💬 4. Test Natural Conversation\n",
      "\n",
      "🚀 Let's give your agents real intelligence!\n",
      "\n",
      "💡 LLM Options (in order of recommendation):\n",
      "   🥇 Hugging Face (free, excellent models) - Unsloth DeepSeek-R1-Distill 4-bit\n",
      "   🥈 Ollama (free, local, powerful) - ollama.ai\n",
      "\n",
      "🔧 Hardware: Optimized for GPU/Colab - using Unsloth's 4-bit quantized model!\n",
      "🚀 We'll auto-detect the best available option!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging for clean, professional output\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "%pip install transformers  --upgrade\n",
    "\n",
    "logger.info(\"🧠 Welcome to LLM-Powered Agents!\")\n",
    "logger.info(\"=\" * 60)\n",
    "\n",
    "# Set up our workspace\n",
    "current_dir = os.getcwd()\n",
    "if 'notebooks' in current_dir:\n",
    "    project_dir = os.path.dirname(current_dir)\n",
    "else:\n",
    "    project_dir = current_dir\n",
    "\n",
    "logger.info(\"🔧 Environment Setup Complete!\")\n",
    "logger.info(f\"📍 Project directory: {project_dir}\")\n",
    "logger.info(f\"📅 Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "logger.info(\"\\n🎯 Today's Journey:\")\n",
    "logger.info(\"   🔄 1. Compare Rule-Based vs LLM Approaches\")\n",
    "logger.info(\"   🧠 2. Build LLM Integration System\") \n",
    "logger.info(\"   🛠️ 3. Create Intelligent Tool Selection\")\n",
    "logger.info(\"   💬 4. Test Natural Conversation\")\n",
    "\n",
    "logger.info(\"\\n🚀 Let's give your agents real intelligence!\")\n",
    "\n",
    "# Note: We'll start with a simple LLM simulation, then show how to integrate real LLMs\n",
    "logger.info(\"\\n💡 LLM Options (in order of recommendation):\")\n",
    "logger.info(\"   🥇 Hugging Face (free, excellent models) - Unsloth DeepSeek-R1-Distill 4-bit\")\n",
    "logger.info(\"   🥈 Ollama (free, local, powerful) - ollama.ai\")\n",
    "logger.info(\"\\n🔧 Hardware: Optimized for GPU/Colab - using Unsloth's 4-bit quantized model!\")\n",
    "logger.info(\"🚀 We'll auto-detect the best available option!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "814c8251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Initializing LLM Model...\n",
      "==================================================\n",
      "🔄 Creating global LLM client...\n",
      "🧠 Initializing LLM: huggingface\n",
      "🤗 Loading Hugging Face model...\n",
      "🔧 Using device: CPU (will be slower)\n",
      "🔄 Loading model: unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit\n",
      "💡 Loading Unsloth's optimized 4-bit quantized version...\n",
      "⚠️ GPU not available, loading in CPU mode (slower)\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Required packages not installed. Run: pip install transformers torch bitsandbytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Sarthak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\metadata\\__init__.py:397\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscover\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 77\u001b[0m, in \u001b[0;36mSimpleLLMClient._init_huggingface\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠️ GPU not available, loading in CPU mode (slower)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set to evaluation mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sarthak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:600\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    599\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    606\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Sarthak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:311\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Sarthak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:4634\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_quantized:\n\u001b[1;32m-> 4634\u001b[0m     config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoHfQuantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_quantization_configs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\n\u001b[0;32m   4636\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4637\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Sarthak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\quantizers\\auto.py:206\u001b[0m, in \u001b[0;36mAutoHfQuantizer.merge_quantization_configs\u001b[1;34m(cls, quantization_config, quantization_config_from_args)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m         quantization_config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoQuantizationConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    210\u001b[0m         quantization_config, (GPTQConfig, AwqConfig, AutoRoundConfig, FbgemmFp8Config, CompressedTensorsConfig)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    213\u001b[0m ):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# special case for GPTQ / AWQ / FbgemmFp8 config collision\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sarthak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\quantizers\\auto.py:132\u001b[0m, in \u001b[0;36mAutoQuantizationConfig.from_dict\u001b[1;34m(cls, quantization_config_dict)\u001b[0m\n\u001b[0;32m    131\u001b[0m target_cls \u001b[38;5;241m=\u001b[39m AUTO_QUANTIZATION_CONFIG_MAPPING[quant_method]\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantization_config_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sarthak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\quantization_config.py:120\u001b[0m, in \u001b[0;36mQuantizationConfigMixin.from_dict\u001b[1;34m(cls, config_dict, return_unused_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03mInstantiates a [`QuantizationConfigMixin`] from a Python dictionary of parameters.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    [`QuantizationConfigMixin`]: The configuration object instantiated from those parameters.\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m to_remove \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Sarthak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\quantization_config.py:508\u001b[0m, in \u001b[0;36mBitsAndBytesConfig.__init__\u001b[1;34m(self, load_in_8bit, load_in_4bit, llm_int8_threshold, llm_int8_skip_modules, llm_int8_enable_fp32_cpu_offload, llm_int8_has_fp16_weight, bnb_4bit_compute_dtype, bnb_4bit_quant_type, bnb_4bit_use_double_quant, bnb_4bit_quant_storage, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnused kwargs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. These kwargs are not used in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 508\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sarthak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\quantization_config.py:566\u001b[0m, in \u001b[0;36mBitsAndBytesConfig.post_init\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbnb_4bit_use_double_quant must be a boolean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_in_4bit \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbitsandbytes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m ):\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    570\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4 bit quantization requires bitsandbytes>=0.39.0 - please upgrade your bitsandbytes version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    571\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Sarthak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\metadata\\__init__.py:889\u001b[0m, in \u001b[0;36mversion\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[0;32m    884\u001b[0m \n\u001b[0;32m    885\u001b[0m \u001b[38;5;124;03m:param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;124;03m:return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;124;03m    \"Version\" metadata key.\u001b[39;00m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n",
      "File \u001b[1;32mc:\\Users\\Sarthak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\metadata\\__init__.py:862\u001b[0m, in \u001b[0;36mdistribution\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \n\u001b[0;32m    859\u001b[0m \u001b[38;5;124;03m:param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;124;03m:return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 862\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sarthak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\metadata\\__init__.py:399\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "\u001b[1;31mPackageNotFoundError\u001b[0m: No package metadata was found for bitsandbytes",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 216\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# 🎯 Initialize the LLM client once for the entire notebook\u001b[39;00m\n\u001b[0;32m    215\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔄 Creating global LLM client...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 216\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleLLMClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuggingface\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Model initialization complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    219\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🚀 Ready for agent development!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 36\u001b[0m, in \u001b[0;36mSimpleLLMClient.__init__\u001b[1;34m(self, backend, model_name)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Initialize based on backend choice\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggingface\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_huggingface\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mollama\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_ollama()\n",
      "Cell \u001b[1;32mIn[6], line 94\u001b[0m, in \u001b[0;36mSimpleLLMClient._init_huggingface\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel failed to load - check GPU memory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequired packages not installed. Run: pip install transformers torch bitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingFace initialization failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: Required packages not installed. Run: pip install transformers torch bitsandbytes"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Initialize logging for model loading\n",
    "logger.info(\"\\n🚀 Initializing LLM Model...\")\n",
    "logger.info(\"=\" * 50)\n",
    "\n",
    "class SimpleLLMClient:\n",
    "    \"\"\"\n",
    "    Simple LLM client that can work with multiple backends.\n",
    "    Supports: OpenAI API, Ollama, or Hugging Face Transformers\n",
    "    \n",
    "    This shows how to integrate real LLMs into agent systems.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, backend=\"huggingface\", model_name=\"auto\"):\n",
    "        \"\"\"\n",
    "        Initialize LLM client with specified backend.\n",
    "        \n",
    "        Args:\n",
    "            backend: \"huggingface\", \"openai\", or \"ollama\"\n",
    "            model_name: Model to use (depends on backend, \"auto\" for best available)\n",
    "        \"\"\"\n",
    "        self.backend = backend\n",
    "        self.model_name = model_name\n",
    "        self.client = None\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
    "        \n",
    "        self.logger.info(f\"🧠 Initializing LLM: {backend}\")\n",
    "        if model_name != \"auto\":\n",
    "            self.logger.info(f\"🎯 Requested model: {model_name}\")\n",
    "        \n",
    "        # Initialize based on backend choice\n",
    "        if backend == \"huggingface\":\n",
    "            self._init_huggingface()\n",
    "        elif backend == \"ollama\":\n",
    "            self._init_ollama()\n",
    "        else:\n",
    "            raise Exception(f\"Unsupported backend: {backend}. Use 'huggingface' or 'ollama'\")\n",
    "    \n",
    "    def _init_huggingface(self):\n",
    "        \"\"\"Initialize Hugging Face transformers (optimized for GPU systems)\"\"\"\n",
    "        try:\n",
    "            from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "            import torch\n",
    "            self.logger.info(\"🤗 Loading Hugging Face model...\")\n",
    "            \n",
    "            # Check GPU availability (works on both local and Colab)\n",
    "            if torch.cuda.is_available():\n",
    "                device = \"cuda\"\n",
    "                self.logger.info(f\"🔧 Using device: GPU (CUDA)\")\n",
    "            else:\n",
    "                device = \"cpu\"\n",
    "                self.logger.info(f\"🔧 Using device: CPU (will be slower)\")\n",
    "            \n",
    "            # Use Unsloth's pre-quantized DeepSeek-R1-Distill model\n",
    "            model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit\"\n",
    "            \n",
    "            self.logger.info(f\"🔄 Loading model: {model_name}\")\n",
    "            self.logger.info(f\"💡 Loading Unsloth's optimized 4-bit quantized version...\")\n",
    "\n",
    "            # 1) Load tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "            \n",
    "            # 2) Load pre-quantized model with Unsloth optimizations\n",
    "            if device == \"cuda\":\n",
    "                # GPU: Use the pre-quantized model as-is\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "            else:\n",
    "                # CPU fallback: Load in float32\n",
    "                self.logger.warning(\"⚠️ GPU not available, loading in CPU mode (slower)\")\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    torch_dtype=torch.float32,\n",
    "                    device_map=None,\n",
    "                    trust_remote_code=True\n",
    "                ).to(device)\n",
    "            \n",
    "            self.model.eval()  # Set to evaluation mode\n",
    "            \n",
    "            self.logger.info(f\"✅ Model loaded successfully!\")\n",
    "            self.model_name = model_name\n",
    "            self.client = \"unsloth_deepseek_loaded\"  # Flag to indicate model is ready\n",
    "                    \n",
    "            if self.model is None:\n",
    "                raise Exception(\"Model failed to load - check GPU memory\")\n",
    "            \n",
    "        except ImportError:\n",
    "            raise Exception(\"Required packages not installed. Run: pip install transformers torch bitsandbytes\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"HuggingFace initialization failed: {e}\")\n",
    "\n",
    "    def _init_ollama(self):\n",
    "        \"\"\"Initialize Ollama local LLM (requires Ollama installed)\"\"\"\n",
    "        try:\n",
    "            import requests\n",
    "            \n",
    "            # Test if Ollama is running\n",
    "            response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                self.client = \"ollama_available\"\n",
    "                self.logger.info(\"✅ Ollama connection established!\")\n",
    "            else:\n",
    "                raise Exception(\"Ollama not responding - make sure it's running\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error connecting to Ollama: {e}. Make sure Ollama is installed and running: https://ollama.ai\")\n",
    "    \n",
    "    def generate_response(self, prompt: str, max_tokens: int = 100) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using the configured LLM backend.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Input prompt for the LLM\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            Generated response from the LLM\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.backend == \"huggingface\":\n",
    "                return self._generate_huggingface(prompt, max_tokens)\n",
    "            elif self.backend == \"ollama\":\n",
    "                return self._generate_ollama(prompt, max_tokens)\n",
    "            else:\n",
    "                raise Exception(f\"Unknown backend: {self.backend}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error generating response: {e}\")\n",
    "    \n",
    "    def _generate_huggingface(self, prompt: str, max_tokens: int) -> str:\n",
    "        \"\"\"Generate response using Unsloth's optimized DeepSeek-R1-Distill model\"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            raise Exception(\"Unsloth DeepSeek model not properly loaded\")\n",
    "        \n",
    "        try:\n",
    "            import torch\n",
    "            \n",
    "            # DeepSeek uses standard chat format\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful AI learning assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "            \n",
    "            # Apply chat template and tokenize\n",
    "            text = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            # Tokenize the formatted text\n",
    "            inputs = self.tokenizer(\n",
    "                text, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=2048\n",
    "            ).to(self.model.device)\n",
    "            \n",
    "            # Generate response with optimized parameters for quantized model\n",
    "            with torch.inference_mode():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=min(max_tokens, 150),\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    repetition_penalty=1.1,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    use_cache=True  # Optimize for quantized model\n",
    "                )\n",
    "            \n",
    "            # Decode only the new tokens (exclude input)\n",
    "            input_length = inputs.input_ids.shape[1]\n",
    "            new_tokens = outputs[0][input_length:]\n",
    "            response = self.tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "            \n",
    "            # Clean up the response\n",
    "            response = response.strip()\n",
    "            \n",
    "            # Remove any residual chat formatting\n",
    "            response = response.replace(\"<|im_end|>\", \"\").replace(\"<|im_start|>\", \"\").strip()\n",
    "            \n",
    "            # Quality check\n",
    "            if len(response) > 5 and not response.lower().startswith(prompt.lower()[:10]):\n",
    "                return response\n",
    "            else:\n",
    "                return \"I'd be happy to help you with that!\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in Unsloth DeepSeek generation: {e}\")\n",
    "    \n",
    "    def _generate_ollama(self, prompt: str, max_tokens: int) -> str:\n",
    "        \"\"\"Generate response using Ollama\"\"\"\n",
    "        import requests\n",
    "        \n",
    "        data = {\n",
    "            \"model\": \"llama3\",  # Default model\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\"http://localhost:11434/api/generate\", json=data)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"response\", \"\").strip()\n",
    "        else:\n",
    "            raise Exception(f\"Ollama request failed with status {response.status_code}\")\n",
    "\n",
    "# 🎯 Initialize the LLM client once for the entire notebook\n",
    "logger.info(\"🔄 Creating global LLM client...\")\n",
    "llm = SimpleLLMClient(backend=\"huggingface\")\n",
    "\n",
    "logger.info(\"✅ Model initialization complete!\")\n",
    "logger.info(\"🚀 Ready for agent development!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e333f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "\n",
    "class SimpleLLMClient:\n",
    "    \"\"\"\n",
    "    Simple LLM client that can work with multiple backends.\n",
    "    Supports: OpenAI API, Ollama, or Hugging Face Transformers\n",
    "    \n",
    "    This shows how to integrate real LLMs into agent systems.\n",
    "    \"\"\"\n",
    "    \n",
    "    _instance = None  # Class variable to store singleton instance\n",
    "    \n",
    "    def __new__(cls, backend=\"huggingface\", model_name=\"auto\"):\n",
    "        # Implement singleton pattern to prevent duplicate initialization\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super(SimpleLLMClient, cls).__new__(cls)\n",
    "            cls._instance._initialized = False\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self, backend=\"huggingface\", model_name=\"auto\"):\n",
    "        \"\"\"\n",
    "        Initialize LLM client with specified backend.\n",
    "        \n",
    "        Args:\n",
    "            backend: \"huggingface\", \"openai\", or \"ollama\"\n",
    "            model_name: Model to use (depends on backend, \"auto\" for best available)\n",
    "        \"\"\"\n",
    "        # Prevent re-initialization\n",
    "        if hasattr(self, '_initialized') and self._initialized:\n",
    "            return\n",
    "            \n",
    "        self.backend = backend\n",
    "        self.model_name = model_name\n",
    "        self.client = None\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
    "        \n",
    "        self.logger.info(f\"🧠 Initializing LLM: {backend}\")\n",
    "        if model_name != \"auto\":\n",
    "            self.logger.info(f\"🎯 Requested model: {model_name}\")\n",
    "        \n",
    "        # Initialize based on backend choice\n",
    "        if backend == \"huggingface\":\n",
    "            self._init_huggingface()\n",
    "        elif backend == \"ollama\":\n",
    "            self._init_ollama()\n",
    "        else:\n",
    "            raise Exception(f\"Unsupported backend: {backend}. Use 'huggingface' or 'ollama'\")\n",
    "            \n",
    "        self._initialized = True\n",
    "    \n",
    "    def _init_huggingface(self):\n",
    "        \"\"\"Initialize Hugging Face transformers (optimized for GPU systems)\"\"\"\n",
    "        try:\n",
    "            from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "            import torch\n",
    "            self.logger.info(\"🤗 Loading Hugging Face model...\")\n",
    "            \n",
    "            # Check GPU availability (works on both local and Colab)\n",
    "            if torch.cuda.is_available():\n",
    "                device = \"cuda\"\n",
    "                self.logger.info(f\"🔧 Using device: GPU (CUDA)\")\n",
    "            else:\n",
    "                device = \"cpu\"\n",
    "                self.logger.info(f\"🔧 Using device: CPU (will be slower)\")\n",
    "            \n",
    "            # Use Unsloth's pre-quantized DeepSeek-R1-Distill model\n",
    "            model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit\"\n",
    "            \n",
    "            self.logger.info(f\"🔄 Loading model: {model_name}\")\n",
    "            self.logger.info(f\"💡 Loading Unsloth's optimized 4-bit quantized version...\")\n",
    "\n",
    "            # 1) Load tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "            \n",
    "            # 2) Load pre-quantized model with Unsloth optimizations\n",
    "            if device == \"cuda\":\n",
    "                # GPU: Use the pre-quantized model as-is\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "            else:\n",
    "                # CPU fallback: Load in float32\n",
    "                self.logger.warning(\"⚠️ GPU not available, loading in CPU mode (slower)\")\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    torch_dtype=torch.float32,\n",
    "                    device_map=None,\n",
    "                    trust_remote_code=True\n",
    "                ).to(device)\n",
    "            \n",
    "            self.model.eval()  # Set to evaluation mode\n",
    "            \n",
    "            self.logger.info(f\"✅ Model loaded successfully!\")\n",
    "            self.model_name = model_name\n",
    "            self.client = \"unsloth_deepseek_loaded\"  # Flag to indicate model is ready\n",
    "                    \n",
    "            if self.model is None:\n",
    "                raise Exception(\"Model failed to load - check GPU memory\")\n",
    "            \n",
    "        except ImportError:\n",
    "            raise Exception(\"Required packages not installed. Run: pip install transformers torch bitsandbytes\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"HuggingFace initialization failed: {e}\")\n",
    "\n",
    "            \n",
    "    def _init_ollama(self):\n",
    "        \"\"\"Initialize Ollama local LLM (requires Ollama installed)\"\"\"\n",
    "        try:\n",
    "            import requests\n",
    "            \n",
    "            # Test if Ollama is running\n",
    "            response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                self.client = \"ollama_available\"\n",
    "                self.logger.info(\"✅ Ollama connection established!\")\n",
    "            else:\n",
    "                raise Exception(\"Ollama not responding - make sure it's running\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error connecting to Ollama: {e}. Make sure Ollama is installed and running: https://ollama.ai\")\n",
    "    \n",
    "    def generate_response(self, prompt: str, max_tokens: int = 100) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using the configured LLM backend.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Input prompt for the LLM\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            Generated response from the LLM\n",
    "        \"\"\"\n",
    "        # Don't print during generation to avoid clutter\n",
    "        # print(f\"🧠 LLM generating response...\")\n",
    "        \n",
    "        try:\n",
    "            if self.backend == \"huggingface\":\n",
    "                return self._generate_huggingface(prompt, max_tokens)\n",
    "            elif self.backend == \"ollama\":\n",
    "                return self._generate_ollama(prompt, max_tokens)\n",
    "            else:\n",
    "                raise Exception(f\"Unknown backend: {self.backend}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error generating response: {e}\")\n",
    "    \n",
    "    def _generate_huggingface(self, prompt: str, max_tokens: int) -> str:\n",
    "        \"\"\"Generate response using Unsloth's optimized DeepSeek-R1-Distill model\"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            raise Exception(\"Unsloth DeepSeek model not properly loaded\")\n",
    "        \n",
    "        try:\n",
    "            import torch\n",
    "            \n",
    "            # DeepSeek uses standard chat format\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful AI learning assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "            \n",
    "            # Apply chat template and tokenize\n",
    "            text = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            # Tokenize the formatted text\n",
    "            inputs = self.tokenizer(\n",
    "                text, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=2048\n",
    "            ).to(self.model.device)\n",
    "            \n",
    "            # Generate response with optimized parameters for quantized model\n",
    "            with torch.inference_mode():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=min(max_tokens, 150),\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    repetition_penalty=1.1,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    use_cache=True  # Optimize for quantized model\n",
    "                )\n",
    "            \n",
    "            # Decode only the new tokens (exclude input)\n",
    "            input_length = inputs.input_ids.shape[1]\n",
    "            new_tokens = outputs[0][input_length:]\n",
    "            response = self.tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "            \n",
    "            # Clean up the response\n",
    "            response = response.strip()\n",
    "            \n",
    "            # Remove any residual chat formatting\n",
    "            response = response.replace(\"<|im_end|>\", \"\").replace(\"<|im_start|>\", \"\").strip()\n",
    "            \n",
    "            # Quality check\n",
    "            if len(response) > 5 and not response.lower().startswith(prompt.lower()[:10]):\n",
    "                return response\n",
    "            else:\n",
    "                return \"I'd be happy to help you with that!\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in Unsloth DeepSeek generation: {e}\")\n",
    "    \n",
    "    def _generate_ollama(self, prompt: str, max_tokens: int) -> str:\n",
    "        \"\"\"Generate response using Ollama\"\"\"\n",
    "        import requests\n",
    "        \n",
    "        data = {\n",
    "            \"model\": \"llama3\",  # Default model\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\"http://localhost:11434/api/generate\", json=data)\n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"response\", \"\").strip()\n",
    "        else:\n",
    "            raise Exception(f\"Ollama request failed with status {response.status_code}\")\n",
    "\n",
    "# 🧪 Test the real LLM integration\n",
    "logger.info(\"\\n🧪 Testing Real LLM Integration\")\n",
    "logger.info(\"-\" * 40)\n",
    "\n",
    "# Create LLM client (will auto-detect best available option)\n",
    "logger.info(\"\\n🔄 Initializing LLM client...\")\n",
    "\n",
    "llm = SimpleLLMClient(backend=\"huggingface\")\n",
    "\n",
    "logger.info(\"\\n🧪 Running tests...\")\n",
    "\n",
    "# Test with educational prompts\n",
    "test_prompts = [\n",
    "    \"Hello! I'm new to programming and want to learn Python.\",\n",
    "    \"Can you explain what a function is in simple terms?\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    logger.info(f\"\\n📝 Test {i}: {prompt}\")\n",
    "    \n",
    "    # Generate response using real LLM\n",
    "    response = llm.generate_response(prompt, max_tokens=150)\n",
    "    \n",
    "    logger.info(f\"🤖 Response: {response}\")\n",
    "\n",
    "logger.info(f\"\\n✅ LLM integration complete! The agent now has genuine AI intelligence.\")\n",
    "logger.info(f\"🚀 Next: We'll integrate this into our agent architecture!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af263b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LLMToolSelector:\n",
    "    \"\"\"\n",
    "    Uses real LLM intelligence to choose the best tools for each task.\n",
    "    This is much smarter than rule-based selection!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client: SimpleLLMClient):\n",
    "        self.llm = llm_client\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
    "        \n",
    "        # Available tools with descriptions (this is what the LLM sees)\n",
    "        self.available_tools = {\n",
    "            'calculator': 'Performs mathematical calculations and computations',\n",
    "            'file_manager': 'Creates, reads, and manages files and documents', \n",
    "            'timer': 'Sets timers and manages time-based activities',\n",
    "            'web_search': 'Searches the internet for current information',\n",
    "            'calendar': 'Manages schedules, appointments, and deadlines'\n",
    "        }\n",
    "    \n",
    "    def select_tools(self, user_request: str, context: str = \"\") -> List[str]:\n",
    "        \"\"\"\n",
    "        Use LLM to intelligently select tools based on user request.\n",
    "        \n",
    "        Args:\n",
    "            user_request: What the user wants to accomplish\n",
    "            context: Additional context about the user or situation\n",
    "            \n",
    "        Returns:\n",
    "            List of tools the LLM recommends using\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"🛠️ Selecting tools for: '{user_request}'\")\n",
    "        \n",
    "        # Create a structured prompt for tool selection\n",
    "        tool_list = \"\\n\".join([f\"- {name}: {desc}\" for name, desc in self.available_tools.items()])\n",
    "        \n",
    "        prompt = f\"\"\"You are an AI agent that helps users by selecting the right tools.\n",
    "\n",
    "User Request: \"{user_request}\"\n",
    "Context: {context}\n",
    "\n",
    "Available Tools:\n",
    "{tool_list}\n",
    "\n",
    "Task: Analyze the user's request and select which tools would be most helpful. Consider:\n",
    "1. What is the user trying to accomplish?\n",
    "2. What information or actions are needed?\n",
    "3. Which tools can provide that?\n",
    "\n",
    "Respond with only the tool names, separated by commas. For example: \"calculator, file_manager\"\n",
    "Only choose the tools that are absolutely necessary for this request. \n",
    "Selected tools:\"\"\"\n",
    "\n",
    "        # Get LLM response\n",
    "        response = self.llm.generate_response(prompt, max_tokens=500)\n",
    "        \n",
    "        # Parse the LLM's tool selection\n",
    "        selected_tools = []\n",
    "        for tool_name in self.available_tools.keys():\n",
    "            if tool_name in response.lower():\n",
    "                selected_tools.append(tool_name)\n",
    "        \n",
    "        self.logger.info(f\"🎯 Selected tools: {selected_tools}\")\n",
    "        return selected_tools\n",
    "    \n",
    "    def explain_selection(self, user_request: str, selected_tools: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Ask the LLM to explain why it selected these tools.\n",
    "        This helps users understand the agent's reasoning!\n",
    "        \"\"\"\n",
    "        if not selected_tools:\n",
    "            return \"No tools were needed for this request.\"\n",
    "        \n",
    "        tools_str = \", \".join(selected_tools)\n",
    "        \n",
    "        prompt = f\"\"\"Explain in one sentence why these tools ({tools_str}) are the best choice for this request: \"{user_request}\"\n",
    "\n",
    "Explanation:\"\"\"\n",
    "        \n",
    "        explanation = self.llm.generate_response(prompt, max_tokens=60)\n",
    "        return explanation.strip()\n",
    "\n",
    "# 🧪 Test LLM-powered tool selection\n",
    "logger.info(\"\\n🧪 Testing LLM Tool Selection\")\n",
    "logger.info(\"-\" * 40)\n",
    "\n",
    "# Create tool selector with our LLM\n",
    "tool_selector = LLMToolSelector(llm)\n",
    "\n",
    "# Test various requests to see intelligent tool selection\n",
    "test_requests = [\n",
    "    \"I need to calculate the compound interest on my savings\",\n",
    "    \"Help me create a study schedule for learning Python\",\n",
    "    \"I want to save my class notes somewhere organized\", \n",
    "    \"What's the latest information about machine learning trends?\",\n",
    "    \"Set up a 25-minute focus session for studying\"\n",
    "]\n",
    "\n",
    "for request in test_requests:\n",
    "    logger.info(f\"\\n📝 User Request: {request}\")\n",
    "    \n",
    "    # Let LLM select tools\n",
    "    selected_tools = tool_selector.select_tools(request)\n",
    "    \n",
    "    # Get explanation\n",
    "    explanation = tool_selector.explain_selection(request, selected_tools)\n",
    "    \n",
    "    logger.info(f\"🤖 Reasoning: {explanation}\")\n",
    "\n",
    "logger.info(f\"\\n✅ Notice how the LLM:\")\n",
    "logger.info(f\"   • Understands the intent behind each request\")\n",
    "logger.info(f\"   • Selects appropriate tools intelligently\")\n",
    "logger.info(f\"   • Can explain its reasoning\")\n",
    "logger.info(f\"   • Handles requests it's never seen before!\")\n",
    "\n",
    "logger.info(f\"\\n🚀 This is the power of LLM-driven agent intelligence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc65d976",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LLMAgent:\n",
    "    \"\"\"\n",
    "    A complete AI agent powered by real LLM intelligence.\n",
    "    This combines all our previous concepts with genuine AI reasoning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str = \"LLMAgent\", llm_client: SimpleLLMClient = None):\n",
    "        self.name = name\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
    "        \n",
    "        # Use provided LLM client or create new one\n",
    "        if llm_client is not None:\n",
    "            self.llm = llm_client\n",
    "            self.logger.info(f\"🤖 {self.name} initialized with shared LLM client!\")\n",
    "        else:\n",
    "            # Initialize LLM and tool systems\n",
    "            self.llm = SimpleLLMClient(backend=\"huggingface\")\n",
    "            self.logger.info(f\"🤖 {self.name} initialized with new LLM client!\")\n",
    "            \n",
    "        self.tool_selector = LLMToolSelector(self.llm)\n",
    "        \n",
    "        # Simple memory for conversation context\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def process_request(self, user_input: str) -> str:\n",
    "        \"\"\"\n",
    "        Process user request using LLM-powered intelligence.\n",
    "        This is the complete agent interaction loop!\n",
    "        \n",
    "        Args:\n",
    "            user_input: What the user said\n",
    "            \n",
    "        Returns:\n",
    "            Intelligent response from the agent\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"🤖 {self.name} processing: '{user_input}'\")\n",
    "        \n",
    "        # STEP 1: 👁️ PERCEPTION - Add to conversation context\n",
    "        self.conversation_history.append(f\"User: {user_input}\")\n",
    "        context = \" \".join(self.conversation_history[-3:])  # Last 3 interactions\n",
    "        \n",
    "        # STEP 2: 🧠 LLM REASONING - Tool selection\n",
    "        self.logger.debug(\"🛠️ Selecting tools...\")\n",
    "        selected_tools = self.tool_selector.select_tools(user_input, context)\n",
    "        \n",
    "        # STEP 3: 🤚 ACTION - Generate response\n",
    "        self.logger.debug(\"💬 Generating response...\")\n",
    "        response = self._generate_contextual_response(user_input, selected_tools, context)\n",
    "        \n",
    "        # STEP 4: 💾 MEMORY - Store interaction\n",
    "        self.conversation_history.append(f\"Agent: {response}\")\n",
    "        \n",
    "        self.logger.info(f\"🤖 {self.name}: {response}\")\n",
    "        return response\n",
    "    \n",
    "    def _generate_contextual_response(self, user_input: str, tools: List[str], context: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a natural response using LLM with tool awareness.\n",
    "        \"\"\"\n",
    "        # Create a comprehensive prompt for natural conversation\n",
    "        tools_info = f\"Available tools: {', '.join(tools)}\" if tools else \"No tools needed\"\n",
    "        \n",
    "        prompt = f\"\"\"You are a helpful AI learning assistant having a conversation.\n",
    "\n",
    "Recent context: {context}\n",
    "Current user message: \"{user_input}\"\n",
    "{tools_info}\n",
    "\n",
    "Respond naturally and helpfully. If tools were selected, mention how you would use them to help. Keep response conversational and encouraging.\n",
    "\n",
    "Response:\"\"\"\n",
    "        \n",
    "        response = self.llm.generate_response(prompt, max_tokens=100)\n",
    "        \n",
    "        # Add tool usage information if tools were selected\n",
    "        if tools:\n",
    "            tool_explanation = self.tool_selector.explain_selection(user_input, tools)\n",
    "            response += f\" I'll use {', '.join(tools)} to help - {tool_explanation.lower()}\"\n",
    "        \n",
    "        return response.strip()\n",
    "\n",
    "# 🧪 Test the complete LLM-powered agent\n",
    "logger.info(\"\\n🧪 Testing Complete LLM Agent\")\n",
    "logger.info(\"-\" * 40)\n",
    "\n",
    "# Create our intelligent agent using the existing LLM client (no duplication!)\n",
    "smart_agent = LLMAgent(\"StudyBuddy\", llm_client=llm)\n",
    "\n",
    "# Have a real conversation with the LLM-powered agent\n",
    "conversation = [\n",
    "    \"Hello! I'm struggling with learning Python programming\",\n",
    "    \"I need help with loops specifically - they're confusing me\",\n",
    "    \"Can you create a study plan for me?\",\n",
    "    \"What's 15% of 240? I need this for my homework\", \n",
    "    \"Thanks! You've been really helpful\"\n",
    "]\n",
    "\n",
    "for message in conversation:\n",
    "    logger.info(f\"\\n👤 User: {message}\")\n",
    "    response = smart_agent.process_request(message)\n",
    "\n",
    "logger.info(f\"\\n🎉 Complete LLM-powered agent success!\")\n",
    "logger.info(f\"✅ Real LLM intelligence\")\n",
    "logger.info(f\"✅ Intelligent tool selection\") \n",
    "logger.info(f\"✅ Natural conversation\")\n",
    "logger.info(f\"✅ Context awareness\")\n",
    "logger.info(f\"✅ Memory of past interactions\")\n",
    "\n",
    "logger.info(f\"\\n💡 This agent can handle ANY request intelligently!\")\n",
    "logger.info(f\"🚀 Ready for the StudyBuddy multi-agent system on Day 4!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
