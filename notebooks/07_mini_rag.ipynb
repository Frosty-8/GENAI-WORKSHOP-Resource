{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87d0925b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import warnings\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if GPU is available (industry standard practice)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caf2804a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dataset loaded successfully!\n",
      "Dataset contains 30 question-answer pairs\n",
      "Handbook text length: 1924 characters\n"
     ]
    }
   ],
   "source": [
    "with open('../data/handbook_text.txt', 'r') as file:\n",
    "    handbook_text = file.read()\n",
    "\n",
    "# Load FAQ dataset from JSON format\n",
    "data_path = '../data/campus_faq.json'\n",
    "\n",
    "try:\n",
    "    with open(data_path) as f:\n",
    "        data = json.load(f)\n",
    "    print(\"‚úì Dataset loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: Could not find the dataset file. Please check the path.\")\n",
    "    raise\n",
    "\n",
    "# Extract questions and answers from the nested structure\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for item in data['faq']:\n",
    "    questions.append(item['question'])\n",
    "    answers.append(item['answer'])\n",
    "\n",
    "# Create DataFrame for easier data manipulation\n",
    "faq_data = pd.DataFrame({\n",
    "    'question': questions,\n",
    "    'answer': answers\n",
    "})\n",
    "\n",
    "print(f\"Dataset contains {len(faq_data)} question-answer pairs\")\n",
    "print(f\"Handbook text length: {len(handbook_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "601de7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing your chunking function...\n",
      "Original: This is sentence one. This is sentence two. This is sentence three. This is sentence four.\n",
      "Chunks: ['This is sentence four']\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks for better retrieval.\n",
    "    This is a simplified version - production systems use more sophisticated methods.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to chunk\n",
    "        chunk_size: Maximum characters per chunk\n",
    "        overlap: Characters to overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # TODO: Split text into sentences (hint: use re.split with pattern r'[.!?]+')\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "            \n",
    "        # TODO: Check if adding this sentence would exceed chunk_size\n",
    "        # If yes AND current_chunk is not empty, start a new chunk\n",
    "        if current_chunk and len(current_chunk) + len(sentence) + 1 > chunk_size:\n",
    "            # Add current_chunk to chunks\n",
    "            chunks.append(current_chunk.strip())\n",
    "            \n",
    "            # Add overlap from the end of previous chunk\n",
    "            if overlap > 0 and len(current_chunk) > overlap:\n",
    "                current_chunk = current_chunk[-overlap:] + \" \" + sentence\n",
    "            else:\n",
    "                current_chunk = sentence\n",
    "        else:\n",
    "            # TODO: Add sentence to current_chunk (handle empty chunk case)\n",
    "            current_chunk = sentence\n",
    "    \n",
    "    # Don't forget the last chunk!\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test your chunking function\n",
    "print(\"üß™ Testing your chunking function...\")\n",
    "sample_text = \"This is sentence one. This is sentence two. This is sentence three. This is sentence four.\"\n",
    "test_chunks = chunk_text(sample_text, chunk_size=50, overlap=10)\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Chunks: {test_chunks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cab0df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking handbook text...\n",
      "‚úì Created 6 handbook chunks\n",
      "Preparing FAQ documents...\n",
      "‚úì Created 30 FAQ documents\n",
      "\n",
      "üìö Knowledge Base Summary:\n",
      "- Total documents: 36\n",
      "- Handbook chunks: 6\n",
      "- FAQ documents: 30\n",
      "- Metadata entries: 36\n",
      "\n",
      "üîç Sample Documents:\n",
      "Handbook chunk example: **Machine Learning Basics**\n",
      "   - Overview of Machine Learning\n",
      "   - Types of Machine Learning: Supervised, Unsupervised, Reinforcement Learning\n",
      "   - Key Algorithms: Linear Regression, Decision Trees, N...\n",
      "FAQ document example: Question: What are the library hours? \n",
      "Answer: The library is open from 8 AM to 10 PM, Monday to Friday....\n"
     ]
    }
   ],
   "source": [
    "print(\"Chunking handbook text...\")\n",
    "# TODO: Use your chunking function to split handbook_text into chunks\n",
    "# Use chunk_size=400 and overlap=50\n",
    "handbook_chunks = (chunk_text(handbook_text,chunk_size=400,overlap=50))\n",
    "print(f\"‚úì Created {len(handbook_chunks)} handbook chunks\")\n",
    "\n",
    "# Step 3b: Format FAQ documents\n",
    "print(\"Preparing FAQ documents...\")\n",
    "faq_documents = []\n",
    "\n",
    "# TODO: Loop through faq_data and create formatted documents\n",
    "# Format each as: \"Question: [question]\\nAnswer: [answer]\"\n",
    "for _, row in faq_data.iterrows():\n",
    "    faq_doc = f\"Question: {row['question']} \\nAnswer: {row['answer']}\"\n",
    "    faq_documents.append(faq_doc)\n",
    "\n",
    "print(f\"‚úì Created {len(faq_documents)} FAQ documents\")\n",
    "\n",
    "# Step 3c: Combine and create metadata\n",
    "all_documents = handbook_chunks + faq_documents\n",
    "document_metadata = []\n",
    "\n",
    "# Create metadata for handbook chunks (this is complete)\n",
    "for i, chunk in enumerate(handbook_chunks):\n",
    "    document_metadata.append({\n",
    "        'source': 'handbook',\n",
    "        'chunk_id': i,\n",
    "        'type': 'text_chunk'\n",
    "    })\n",
    "\n",
    "# TODO: Create metadata for FAQ documents\n",
    "for i, faq_doc in enumerate(faq_documents):\n",
    "    metadata = {\n",
    "        'source':'faq',\n",
    "        'faq_id':'i',\n",
    "        'type':'qa_pair',\n",
    "        'question':faq_data.iloc[i]['question']\n",
    "        # Include: 'source': 'faq', 'faq_id': i, 'type': 'qa_pair', 'question': faq_data.iloc[i]['question']\n",
    "    }\n",
    "    document_metadata.append(metadata)\n",
    "\n",
    "print(f\"\\nüìö Knowledge Base Summary:\")\n",
    "print(f\"- Total documents: {len(all_documents)}\")\n",
    "print(f\"- Handbook chunks: {len(handbook_chunks)}\")\n",
    "print(f\"- FAQ documents: {len(faq_documents)}\")\n",
    "print(f\"- Metadata entries: {len(document_metadata)}\")\n",
    "\n",
    "# Let's examine a few examples\n",
    "print(f\"\\nüîç Sample Documents:\")\n",
    "if handbook_chunks:\n",
    "    print(f\"Handbook chunk example: {handbook_chunks[0][:200]}...\")\n",
    "if faq_documents:\n",
    "    print(f\"FAQ document example: {faq_documents[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cae7230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading sentence transformer model...\n",
      "‚úì Model loaded successfully!\n",
      "\n",
      "üßÆ Creating embeddings for 36 documents...\n",
      "This might take a moment - we're converting text to mathematical vectors!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e903bad64c4f22b6660864b93da9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Document embeddings created!\n",
      "üìä Embedding Statistics:\n",
      "   - Shape: (36, 384)\n",
      "   - Each document ‚Üí 384 numbers\n",
      "   - Memory usage: ~0.1 MB\n",
      "\n",
      "üî¨ Embedding Analysis:\n",
      "First document embedding (first 10 values): [-4.5584787e-02 -8.5991524e-02  4.0799651e-02 -4.6326522e-02\n",
      "  1.3739467e-02  3.0480000e-05 -5.2691088e-03 -5.7575550e-02\n",
      " -1.3184093e-01  2.9281942e-02]\n",
      "Embedding range: -0.211 to 0.217\n",
      "Similarity between first two documents: 0.638\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß Loading sentence transformer model...\")\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"‚úì Model loaded successfully!\")\n",
    "\n",
    "# TODO: Create embeddings for all documents\n",
    "print(f\"\\nüßÆ Creating embeddings for {len(all_documents)} documents...\")\n",
    "print(\"This might take a moment - we're converting text to mathematical vectors!\")\n",
    "\n",
    "# YOUR CODE HERE: Use sentence_model.encode() to create embeddings\n",
    "# Hint: Use show_progress_bar=True to see progress\n",
    "document_embeddings = (sentence_model.encode(all_documents,show_progress_bar=True))\n",
    "\n",
    "print(f\"‚úì Document embeddings created!\")\n",
    "print(f\"üìä Embedding Statistics:\")\n",
    "print(f\"   - Shape: {document_embeddings.shape}\")\n",
    "print(f\"   - Each document ‚Üí {document_embeddings.shape[1]} numbers\")\n",
    "print(f\"   - Memory usage: ~{document_embeddings.nbytes / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# üß™ Let's explore what embeddings look like\n",
    "print(f\"\\nüî¨ Embedding Analysis:\")\n",
    "print(f\"First document embedding (first 10 values): {document_embeddings[0][:10]}\")\n",
    "print(f\"Embedding range: {document_embeddings.min():.3f} to {document_embeddings.max():.3f}\")\n",
    "\n",
    "# TODO: Calculate similarity between first two documents\n",
    "# Hint: Use cosine_similarity([document_embeddings[0]], [document_embeddings[1]])[0][0]\n",
    "similarity = cosine_similarity([document_embeddings[0]],[document_embeddings[1]])[0][0]\n",
    "print(f\"Similarity between first two documents: {similarity:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8a3ebf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Your Retrieval Function\n",
      "========================================\n",
      "üîç RETRIEVAL PROCESS for: 'What are the library hours?'\n",
      "==================================================\n",
      "Step 1: Converting query to embedding...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033ac2eb19724381b9817aebfebff04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Query converted to 384-dimensional vector\n",
      "Step 2: Calculating similarities with all documents...\n",
      "‚úì Calculated 36 similarity scores\n",
      "   Similarity range: -0.042 to 0.798\n",
      "Step 3: Finding top-3 most relevant documents...\n",
      "‚úì Top document indices: [ 6 33 17]\n",
      "Step 4: Filtering by threshold (0.1)...\n",
      "‚úì 3 documents passed the threshold\n",
      "\n",
      "üìã RETRIEVAL RESULTS:\n",
      "  1. [FAQ] Score: 0.798\n",
      "      Preview: Question: What are the library hours? \n",
      "Answer: The library is open from 8 AM to ...\n",
      "\n",
      "  2. [FAQ] Score: 0.554\n",
      "      Preview: Question: Where can I study late at night? \n",
      "Answer: The library has 24-hour stud...\n",
      "\n",
      "  3. [FAQ] Score: 0.482\n",
      "      Preview: Question: What are the dining hours? \n",
      "Answer: The cafeteria is open from 7 AM to...\n",
      "\n",
      "Retrieved 3 documents for: 'What are the library hours?'\n",
      "\n",
      "üîç RETRIEVAL PROCESS for: 'How do I register for classes?'\n",
      "==================================================\n",
      "Step 1: Converting query to embedding...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937d40cdfbc94410b88adaee29280ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Query converted to 384-dimensional vector\n",
      "Step 2: Calculating similarities with all documents...\n",
      "‚úì Calculated 36 similarity scores\n",
      "   Similarity range: 0.063 to 0.562\n",
      "Step 3: Finding top-3 most relevant documents...\n",
      "‚úì Top document indices: [18 15 21]\n",
      "Step 4: Filtering by threshold (0.1)...\n",
      "‚úì 3 documents passed the threshold\n",
      "\n",
      "üìã RETRIEVAL RESULTS:\n",
      "  1. [FAQ] Score: 0.562\n",
      "      Preview: Question: How do I add or drop a course? \n",
      "Answer: You can add or drop courses th...\n",
      "\n",
      "  2. [FAQ] Score: 0.503\n",
      "      Preview: Question: What should I do if I have a question about my classes? \n",
      "Answer: Conta...\n",
      "\n",
      "  3. [FAQ] Score: 0.418\n",
      "      Preview: Question: How do I get a student ID card? \n",
      "Answer: New student ID cards can be o...\n",
      "\n",
      "Retrieved 3 documents for: 'How do I register for classes?'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def retrieve_relevant_documents(query: str, top_k: int = 3, similarity_threshold: float = 0.1):\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant documents for a given query.\n",
    "    This function demonstrates the retrieval process step-by-step.\n",
    "    \"\"\"\n",
    "    print(f\"üîç RETRIEVAL PROCESS for: '{query}'\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Convert query to embedding\n",
    "    print(\"Step 1: Converting query to embedding...\")\n",
    "    # TODO: Create embedding for the query using sentence_model\n",
    "    query_embedding = sentence_model.encode([query],show_progress_bar=True)\n",
    "    print(f\"‚úì Query converted to {query_embedding.shape[1]}-dimensional vector\")\n",
    "    \n",
    "    # Step 2: Calculate similarities\n",
    "    print(\"Step 2: Calculating similarities with all documents...\")\n",
    "    # TODO: Calculate cosine similarities between query and all documents\n",
    "    similarities = cosine_similarity(query_embedding,document_embeddings)\n",
    "    similarities = similarities[0]  # Extract the array\n",
    "    print(f\"‚úì Calculated {len(similarities)} similarity scores\")\n",
    "    print(f\"   Similarity range: {similarities.min():.3f} to {similarities.max():.3f}\")\n",
    "    \n",
    "    # Step 3: Find top-k most similar documents\n",
    "    print(f\"Step 3: Finding top-{top_k} most relevant documents...\")\n",
    "    # TODO: Get indices of top-k highest similarity scores\n",
    "    # Hint: Use np.argsort(similarities)[::-1][:top_k]\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    print(f\"‚úì Top document indices: {top_indices}\")\n",
    "    \n",
    "    # Step 4: Filter by threshold and prepare results\n",
    "    print(f\"Step 4: Filtering by threshold ({similarity_threshold})...\")\n",
    "    results = []\n",
    "    \n",
    "    for idx in top_indices:\n",
    "        score = similarities[idx]\n",
    "        # TODO: Check if score meets threshold\n",
    "        if score>= similarity_threshold:\n",
    "            # Prepare result dictionary\n",
    "            result = {\n",
    "                'document': all_documents[idx],\n",
    "                'score': float(score),\n",
    "                'metadata': document_metadata[idx],\n",
    "                'index': int(idx)\n",
    "            }\n",
    "            results.append(result)\n",
    "    \n",
    "    print(f\"‚úì {len(results)} documents passed the threshold\")\n",
    "    \n",
    "    # Step 5: Display results for learning\n",
    "    print(f\"\\nüìã RETRIEVAL RESULTS:\")\n",
    "    for i, result in enumerate(results):\n",
    "        source = result['metadata']['source']\n",
    "        score = result['score']\n",
    "        preview = result['document'][:80] + \"...\" if len(result['document']) > 80 else result['document']\n",
    "        print(f\"  {i+1}. [{source.upper()}] Score: {score:.3f}\")\n",
    "        print(f\"      Preview: {preview}\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# üß™ Test your retrieval function\n",
    "print(\"üß™ Testing Your Retrieval Function\")\n",
    "print(\"=\" * 40)\n",
    "test_queries = [\n",
    "    \"What are the library hours?\",\n",
    "    \"How do I register for classes?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    retrieved_docs = retrieve_relevant_documents(query, top_k=3)\n",
    "    print(f\"Retrieved {len(retrieved_docs)} documents for: '{query}'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c42e3416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Loading GPT-2 model for text generation...\n",
      "‚úì Generation model loaded successfully!\n",
      "Model running on: cpu\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 model and tokenizer for response generation\n",
    "print(\"ü§ñ Loading GPT-2 model for text generation...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "generation_model = GPT2LMHeadModel.from_pretrained('gpt2-medium').to(device)\n",
    "print(\"‚úì Generation model loaded successfully!\")\n",
    "print(f\"Model running on: {next(generation_model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e7ec5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Testing Your Complete RAG System!\n",
      "If you've implemented everything correctly, this should work:\n",
      "\n",
      "üéØ RAG PIPELINE for: 'What are the library hours?'\n",
      "============================================================\n",
      "STEP 1: RETRIEVAL\n",
      "üîç RETRIEVAL PROCESS for: 'What are the library hours?'\n",
      "==================================================\n",
      "Step 1: Converting query to embedding...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc10f967de69479ab06238e352910eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Query converted to 384-dimensional vector\n",
      "Step 2: Calculating similarities with all documents...\n",
      "‚úì Calculated 36 similarity scores\n",
      "   Similarity range: -0.042 to 0.798\n",
      "Step 3: Finding top-3 most relevant documents...\n",
      "‚úì Top document indices: [ 6 33 17]\n",
      "Step 4: Filtering by threshold (0.1)...\n",
      "‚úì 3 documents passed the threshold\n",
      "\n",
      "üìã RETRIEVAL RESULTS:\n",
      "  1. [FAQ] Score: 0.798\n",
      "      Preview: Question: What are the library hours? \n",
      "Answer: The library is open from 8 AM to ...\n",
      "\n",
      "  2. [FAQ] Score: 0.554\n",
      "      Preview: Question: Where can I study late at night? \n",
      "Answer: The library has 24-hour stud...\n",
      "\n",
      "  3. [FAQ] Score: 0.482\n",
      "      Preview: Question: What are the dining hours? \n",
      "Answer: The cafeteria is open from 7 AM to...\n",
      "\n",
      "STEP 2: CONTEXT PREPARATION\n",
      "‚úì Context prepared (423 characters)\n",
      "STEP 3: PROMPT ENGINEERING\n",
      "‚úì Prompt created (578 characters)\n",
      "STEP 4: RESPONSE GENERATION\n",
      "STEP 5: RESPONSE EXTRACTION\n",
      "‚úì Response generated (534 characters)\n",
      "\n",
      "üéâ FINAL ANSWER: The Obama administration is threatening to take unilateral action to block the Keystone XL pipeline because it wants to avoid being \"unilateralized\" with Ottawa, according to a recent report from EnergyWire.\n",
      "\n",
      "The Obama administration is threatening to take unilateral action to block the Keystone XL pipeline because it wants to avoid being \"unilateralized\" with Ottawa, according to a recent report from EnergyWire.\n",
      "\n",
      "The report comes as Obama's administration ramps up its push to win approval for the controversial pipeline to carry\n"
     ]
    }
   ],
   "source": [
    "def generate_rag_response(query: str, max_new_tokens: int = 100):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: retrieve relevant documents and generate response.\n",
    "    üõ†Ô∏è YOUR TASK: Complete the missing parts of this function!\n",
    "    \"\"\"\n",
    "    print(f\"\\nüéØ RAG PIPELINE for: '{query}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Retrieve relevant documents\n",
    "    print(\"STEP 1: RETRIEVAL\")\n",
    "    retrieved_docs = retrieve_relevant_documents(query, top_k=3)\n",
    "    \n",
    "    # Step 2: Prepare context\n",
    "    print(\"STEP 2: CONTEXT PREPARATION\")\n",
    "    if not retrieved_docs:\n",
    "        print(\"‚ö†Ô∏è No relevant documents found - generating without context\")\n",
    "        context = \"No specific information found in the knowledge base.\"\n",
    "    else:\n",
    "        # TODO: Combine retrieved documents into context\n",
    "        context_parts = []\n",
    "        for doc in retrieved_docs:\n",
    "            source_label = f\"[{doc['metadata']['source'].upper()}]\"\n",
    "            # YOUR CODE HERE: Add formatted document to context_parts\n",
    "            # Format: f\"{source_label} {doc['document']}\"\n",
    "            context_parts.append(f\"{source_label} {doc['document']}\")\n",
    "\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    print(f\"‚úì Context prepared ({len(context)} characters)\")\n",
    "    \n",
    "    # Step 3: Create prompt\n",
    "    print(\"STEP 3: PROMPT ENGINEERING\")\n",
    "    # TODO: Create a well-structured prompt\n",
    "    # Include: context, user question, and clear instructions\n",
    "    prompt = f\"\"\"Based on the following context, please answer the user's question accurately and helpfully.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    print(f\"‚úì Prompt created ({len(prompt)} characters)\")\n",
    "    \n",
    "    # Step 4: Generate response\n",
    "    print(\"STEP 4: RESPONSE GENERATION\")\n",
    "    # TODO: Tokenize the prompt and move to device\n",
    "    inputs = tokenizer.encode(prompt,return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # TODO: Generate response using the model\n",
    "        outputs = generation_model.generate(\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        # Include: max_new_tokens, temperature=0.7, do_sample=True, pad_token_id, eos_token_id\n",
    "    \n",
    "    # Step 5: Extract and clean response\n",
    "    print(\"STEP 5: RESPONSE EXTRACTION\")\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # TODO: Extract just the generated part (after the prompt)\n",
    "    generated_answer = full-response.split('Answer:')[1].strip() if 'Answer:' in full_response else full_response.strip()\n",
    "    \n",
    "    print(f\"‚úì Response generated ({len(generated_answer)} characters)\")\n",
    "    \n",
    "    return {\n",
    "        'query': query,\n",
    "        'retrieved_documents': retrieved_docs,\n",
    "        'context_used': context,\n",
    "        'generated_answer': generated_answer,\n",
    "        'num_docs_retrieved': len(retrieved_docs)\n",
    "    }\n",
    "\n",
    "# üß™ Test your complete RAG system!\n",
    "print(\"üöÄ Testing Your Complete RAG System!\")\n",
    "print(\"If you've implemented everything correctly, this should work:\")\n",
    "result = generate_rag_response(\"What are the library hours?\")\n",
    "print(f\"\\nüéâ FINAL ANSWER: {result['generated_answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2f364ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ COMPREHENSIVE RAG SYSTEM TESTING\n",
      "============================================================\n",
      "üõ†Ô∏è YOUR TASK: Uncomment and run the testing code once everything is implemented!\n",
      "\n",
      "üéØ ANALYSIS QUESTIONS (Discuss with your team):\n",
      "- Which queries worked best? Why?\n",
      "- Where did the system struggle? What could be the reasons?\n",
      "- How could you improve the chunking strategy?\n",
      "- What about the prompts could be enhanced?\n",
      "- How might you handle queries with no relevant context?\n",
      "- What would you change for a production system?\n",
      "\n",
      "üèÖ BONUS CHALLENGES:\n",
      "1. Implement a similarity threshold that adapts based on query complexity\n",
      "2. Add a re-ranking step that considers document diversity\n",
      "3. Implement caching for embeddings to speed up repeated queries\n",
      "4. Add evaluation metrics to measure RAG system quality\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "    \"What are the library hours?\",\n",
    "    \"How do I register for classes?\", \n",
    "    \"What dining options are available on campus?\",\n",
    "    \"Tell me about student support services\",\n",
    "    \"What happens if I lose my student ID?\",\n",
    "    \"How can I get involved in campus activities?\"\n",
    "]\n",
    "\n",
    "print(\"üèÜ COMPREHENSIVE RAG SYSTEM TESTING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üõ†Ô∏è YOUR TASK: Uncomment and run the testing code once everything is implemented!\")\n",
    "\n",
    "# TODO: Uncomment the code below once you've completed all the functions\n",
    "\n",
    "\"\"\"\n",
    "all_results = []\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nüìù TEST {i}: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    result = generate_rag_response(query)\n",
    "    all_results.append(result)\n",
    "    \n",
    "    print(f\"\\nüìä RESULTS SUMMARY:\")\n",
    "    print(f\"- Documents retrieved: {result['num_docs_retrieved']}\")\n",
    "    print(f\"- Context length: {len(result['context_used'])} characters\")\n",
    "    print(f\"- Generated answer: {result['generated_answer']}\")\n",
    "    \n",
    "    if i < len(test_queries):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "print(f\"\\n‚úÖ Testing complete! \")\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüéØ ANALYSIS QUESTIONS (Discuss with your team):\")\n",
    "print(\"- Which queries worked best? Why?\")\n",
    "print(\"- Where did the system struggle? What could be the reasons?\") \n",
    "print(\"- How could you improve the chunking strategy?\")\n",
    "print(\"- What about the prompts could be enhanced?\")\n",
    "print(\"- How might you handle queries with no relevant context?\")\n",
    "print(\"- What would you change for a production system?\")\n",
    "\n",
    "print(f\"\\nüèÖ BONUS CHALLENGES:\")\n",
    "print(\"1. Implement a similarity threshold that adapts based on query complexity\")\n",
    "print(\"2. Add a re-ranking step that considers document diversity\")\n",
    "print(\"3. Implement caching for embeddings to speed up repeated queries\")\n",
    "print(\"4. Add evaluation metrics to measure RAG system quality\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
