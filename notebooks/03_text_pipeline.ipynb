{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db2a5e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d063df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Exploring Tokenization:\n",
      "==================================================\n",
      "\n",
      "Original: Hello world!\n",
      "Tokens: ['Hello', 'ƒ†world', '!']\n",
      "Token IDs: [15496, 995, 0]\n",
      "Number of tokens: 3\n",
      "\n",
      "Original: The quick brown fox jumps over the lazy dog.\n",
      "Tokens: ['The', 'ƒ†quick', 'ƒ†brown', 'ƒ†fox', 'ƒ†jumps', 'ƒ†over', 'ƒ†the', 'ƒ†lazy', 'ƒ†dog', '.']\n",
      "Token IDs: [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13]\n",
      "Number of tokens: 10\n",
      "\n",
      "Original: Artificial intelligence is revolutionizing technology.\n",
      "Tokens: ['Art', 'ificial', 'ƒ†intelligence', 'ƒ†is', 'ƒ†revolution', 'izing', 'ƒ†technology', '.']\n",
      "Token IDs: [8001, 9542, 4430, 318, 5854, 2890, 3037, 13]\n",
      "Number of tokens: 8\n",
      "\n",
      "Original: GPT-2 uses transformer architecture.\n",
      "Tokens: ['G', 'PT', '-', '2', 'ƒ†uses', 'ƒ†transformer', 'ƒ†architecture', '.']\n",
      "Token IDs: [38, 11571, 12, 17, 3544, 47385, 10959, 13]\n",
      "Number of tokens: 8\n",
      "\n",
      "Original: Supercalifragilisticexpialidocious\n",
      "Tokens: ['Super', 'cal', 'if', 'rag', 'il', 'ist', 'ice', 'xp', 'ial', 'id', 'ocious']\n",
      "Token IDs: [12442, 9948, 361, 22562, 346, 396, 501, 42372, 498, 312, 32346]\n",
      "Number of tokens: 11\n",
      "\n",
      "üìä Tokenizer vocabulary size: 50257\n"
     ]
    }
   ],
   "source": [
    "# Hint: Use GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Test sentences to explore tokenization\n",
    "test_sentences = [\n",
    "    \"Hello world!\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is revolutionizing technology.\",\n",
    "    \"GPT-2 uses transformer architecture.\",\n",
    "    \"Supercalifragilisticexpialidocious\"  # Long word to see sub-word tokenization\n",
    "]\n",
    "\n",
    "print(\"üîç Exploring Tokenization:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    # TODO: Tokenize the sentence\n",
    "    # Hint: Use tokenizer.encode() to get token IDs\n",
    "    # Use tokenizer.tokenize() to see the actual tokens\n",
    "    tokens = tokenizer.tokenize(sentence)  # Get the actual token strings\n",
    "    token_ids = tokenizer.encode(sentence)  # Get the numerical IDsa\n",
    "    \n",
    "    print(f\"\\nOriginal: {sentence}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    print(f\"Number of tokens: {len(tokens)}\")\n",
    "\n",
    "# TODO: Print tokenizer vocabulary size\n",
    "print(f\"\\nüìä Tokenizer vocabulary size: {len(tokenizer)}\")  # Hint: len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "206b3a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Token Pattern Analysis:\n",
      "============================================================\n",
      "running                        ‚Üí ['running'] (1 tokens)\n",
      "runner                         ‚Üí ['runner'] (1 tokens)\n",
      "run                            ‚Üí ['run'] (1 tokens)\n",
      "unhappiness                    ‚Üí ['un', 'h', 'appiness'] (3 tokens)\n",
      "ChatGPT                        ‚Üí ['Chat', 'G', 'PT'] (3 tokens)\n",
      "COVID-19                       ‚Üí ['CO', 'VID', '-', '19'] (4 tokens)\n",
      "2023                           ‚Üí ['20', '23'] (2 tokens)\n",
      "programming                    ‚Üí ['program', 'ming'] (2 tokens)\n",
      "antidisestablishmentarianism   ‚Üí ['ant', 'idis', 'establishment', 'arian', 'ism'] (5 tokens)\n",
      "\n",
      "üìä Average characters per token: 4.12\n",
      "üìä Longest word in tokens: antidisestablishmentarianism\n"
     ]
    }
   ],
   "source": [
    "analysis_texts = [\n",
    "    \"running\",\n",
    "    \"runner\",\n",
    "    \"run\",\n",
    "    \"unhappiness\",\n",
    "    \"ChatGPT\",\n",
    "    \"COVID-19\",\n",
    "    \"2023\",\n",
    "    \"programming\",\n",
    "    \"antidisestablishmentarianism\"\n",
    "]\n",
    "\n",
    "print(\"üîç Token Pattern Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "token_analysis = []\n",
    "\n",
    "for text in analysis_texts:\n",
    "    tokens = tokenizer.tokenize(text)              # Tokenize the current text\n",
    "    token_count = len(tokens)                      # Count the tokens\n",
    "    chars_per_token = len(text) / token_count      # Average characters per token\n",
    "\n",
    "    token_analysis.append({\n",
    "        'text': text,\n",
    "        'tokens': tokens,\n",
    "        'token_count': token_count,\n",
    "        'chars_per_token': chars_per_token\n",
    "    })\n",
    "\n",
    "    print(f\"{text:30} ‚Üí {tokens} ({token_count} tokens)\")\n",
    "\n",
    "# Create DataFrame and analyze patterns\n",
    "df = pd.DataFrame(token_analysis)\n",
    "\n",
    "avg_chars_per_token = df['chars_per_token'].mean()\n",
    "longest_word = df.loc[df['token_count'].idxmax(), 'text']\n",
    "\n",
    "print(f\"\\nüìä Average characters per token: {avg_chars_per_token:.2f}\")\n",
    "print(f\"üìä Longest word in tokens: {longest_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3d01037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading GPT-2 model (this may take a moment)...\n",
      "‚úÖ GPT-2 model loaded successfully!\n",
      "\n",
      "üèóÔ∏è Model Architecture:\n",
      "Model type: GPT2LMHeadModel\n",
      "Total parameters: 124,439,808\n",
      "Model size: ~124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"üîÑ Loading GPT-2 model (this may take a moment)...\")\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úÖ GPT-2 model loaded successfully!\")\n",
    "\\\n",
    "print(\"\\nüèóÔ∏è Model Architecture:\")\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Model size: ~{total_params / 1e6:.1f}M parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "310c6e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Model Structure Analysis:\n",
      "==================================================\n",
      "Model configuration: GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.46.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Vocabulary size: 50257\n",
      "Maximum sequence length: 1024\n",
      "Number of transformer layers: 12\n",
      "Number of attention heads: 12\n",
      "Hidden size: 768\n",
      "\n",
      "ü§î Comparison to Your Neural Network:\n",
      "Your network had: 2 inputs ‚Üí 4 hidden ‚Üí 1 output\n",
      "GPT-2 has: 50257 inputs ‚Üí 768 hidden ‚Üí 50257 outputs\n",
      "Your network: ~50 parameters\n",
      "GPT-2: 124,439,808 parameters\n",
      "GPT-2 is ~2,488,796x larger!\n"
     ]
    }
   ],
   "source": [
    "# Explore model structure\n",
    "print(\"üîç Model Structure Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: Print model configuration\n",
    "# Hint: Use model.config\n",
    "config = model.config\n",
    "print(f\"Model configuration: {config}\")\n",
    "\n",
    "print(f\"Vocabulary size: {config.vocab_size}\")\n",
    "print(f\"Maximum sequence length: {config.n_positions}\")\n",
    "print(f\"Number of transformer layers: {config.n_layer}\")\n",
    "print(f\"Number of attention heads: {config.n_head}\")\n",
    "print(f\"Hidden size: {config.n_embd}\")\n",
    "\n",
    "# Compare to your simple network\n",
    "print(\"\\nü§î Comparison to Your Neural Network:\")\n",
    "print(f\"Your network had: 2 inputs ‚Üí 4 hidden ‚Üí 1 output\")\n",
    "print(f\"GPT-2 has: {config.vocab_size} inputs ‚Üí {config.n_embd} hidden ‚Üí {config.vocab_size} outputs\")\n",
    "print(f\"Your network: ~50 parameters\")\n",
    "print(f\"GPT-2: {total_params:,} parameters\")\n",
    "print(f\"GPT-2 is ~{total_params/50:,.0f}x larger!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d3d8de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Base prompt: 'In the future, artificial intelligence will'\n",
      "============================================================\n",
      "In the future, artificial intelligence will become a part of people's lives. More than 50,000 applications being developed in Europe, Asia and the United States, totaling more than 5 million developers each year, are already being marketed by large online entities to\n"
     ]
    }
   ],
   "source": [
    "# Load text-generation pipeline using your model and tokenizer\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Base prompt for experiments\n",
    "base_prompt = \"In the future, artificial intelligence will\"\n",
    "\n",
    "print(f\"ü§ñ Base prompt: '{base_prompt}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "output = generator(base_prompt, max_length=50, num_return_sequences=1)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed4a72d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå°Ô∏è Temperature Experiments:\n",
      "==================================================\n",
      "\n",
      "üî• Temperature: 0.1\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the future, artificial intelligence will be able to do things like search for people, find people, find people, find people, find people, find people, find people, find people, find people, find people, find people, find people, find people, find people, find people, find\n",
      "\n",
      "üî• Temperature: 0.7\n",
      "------------------------------\n",
      "In the future, artificial intelligence will be the main driver of our ability to solve complex problems, including our decision-making. We've already seen this with the AI revolution, but with machine learning, we can apply this to our products.\n",
      "\n",
      "We've already tried out our new machine learning solution\n",
      "\n",
      "üî• Temperature: 1.0\n",
      "------------------------------\n",
      "In the future, artificial intelligence will become almost ubiquitous, but it won't be just for computers. Machines will build their own self-driving cars as soon as humans, robots, and the natural world agree on how to make smarter choices.\n",
      "\n",
      "The problem is the human and machine are inseparable\n",
      "\n",
      "üî• Temperature: 1.5\n",
      "------------------------------\n",
      "In the future, artificial intelligence will replace us without hesitation.\n",
      "\n",
      "(Image Credit: Flickr users ryan kasheen)\n",
      "\n",
      "ü§î Discussion Questions:\n",
      "‚Ä¢ Which temperature produced the most coherent text?\n",
      "‚Ä¢ Which was most creative/surprising?\n",
      "‚Ä¢ When might you use each temperature setting?\n"
     ]
    }
   ],
   "source": [
    "# Experiment with different temperatures\n",
    "temperatures = [0.1, 0.7, 1.0, 1.5]\n",
    "\n",
    "print(\"üå°Ô∏è Temperature Experiments:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nüî• Temperature: {temp}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Generate text with different temperatures using the base_prompt and generator\n",
    "    result = generator(\n",
    "        base_prompt,          # prompt\n",
    "        max_length=60,        # try 60\n",
    "        temperature=temp,     # use the current temp variable\n",
    "        do_sample=True,       # enable sampling\n",
    "        pad_token_id=tokenizer.eos_token_id  # for padding\n",
    "    )\n",
    "    \n",
    "    # Extract and print the generated text\n",
    "    generated_text = result[0]['generated_text']\n",
    "    print(generated_text)\n",
    "    \n",
    "print(\"\\nü§î Discussion Questions:\")\n",
    "print(\"‚Ä¢ Which temperature produced the most coherent text?\")\n",
    "print(\"‚Ä¢ Which was most creative/surprising?\")\n",
    "print(\"‚Ä¢ When might you use each temperature setting?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05ef86fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Top-p Sampling Experiments:\n",
      "==================================================\n",
      "\n",
      "üé≤ Top-p: 0.3\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the future, artificial intelligence will be able to learn from our past, and we will be able to learn from our future.\n",
      "\n",
      "The future is bright.\n",
      "\n",
      "The future is bright.\n",
      "\n",
      "The future is bright.\n",
      "\n",
      "The future is bright.\n",
      "\n",
      "The future is bright\n",
      "\n",
      "üé≤ Top-p: 0.7\n",
      "------------------------------\n",
      "In the future, artificial intelligence will be able to better understand what people are saying, what they're saying, what they're saying, and what they're saying.\n",
      "\n",
      "\"We'll be able to better understand what people are saying, what they're saying, what they're saying, and what\n",
      "\n",
      "üé≤ Top-p: 0.9\n",
      "------------------------------\n",
      "In the future, artificial intelligence will make it easier for our children to learn, and easier for our society to understand.\n",
      "\n",
      "In the long run, we should be able to control the robots. But what about the future?\n",
      "\n",
      "We can't control the future without artificial intelligence. But we\n",
      "\n",
      "üé≤ Top-p: 1.0\n",
      "------------------------------\n",
      "In the future, artificial intelligence will be used to answer the \"how\" questions about how our brains communicate.\n",
      "\n",
      "The problem is that it doesn't appear even close.\n",
      "\n",
      "\"As a scientist, I still think artificial intelligence, where we can do things that would actually benefit humans, is\n",
      "\n",
      "ü§î Discussion Questions:\n",
      "‚Ä¢ How did the outputs change with different top-p values?\n",
      "‚Ä¢ What's the trade-off between diversity and quality?\n"
     ]
    }
   ],
   "source": [
    "# Experiment with top-p sampling\n",
    "top_p_values = [0.3, 0.7, 0.9, 1.0]\n",
    "\n",
    "print(\"üéØ Top-p Sampling Experiments:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for top_p in top_p_values:\n",
    "    print(f\"\\nüé≤ Top-p: {top_p}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # TODO: Generate text with different top-p values\n",
    "    result = generator(\n",
    "        base_prompt,\n",
    "        max_length=60,\n",
    "        temperature=0.8,  # Keep temperature constant\n",
    "        top_p=top_p,  # Use the top_p variable\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_text = result[0]['generated_text']\n",
    "    print(generated_text)\n",
    "    \n",
    "print(\"\\nü§î Discussion Questions:\")\n",
    "print(\"‚Ä¢ How did the outputs change with different top-p values?\")\n",
    "print(\"‚Ä¢ What's the trade-off between diversity and quality?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0a25d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úçÔ∏è Prompt Engineering Experiments:\n",
      "============================================================\n",
      "\n",
      "üìù Style: Direct\n",
      "Prompt: 'Write about artificial intelligence:'\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the future, artificial intelligence will have to learn how to use all of the information in your mind and body, and how to use it to better serve your needs.\n",
      "\n",
      "It's only a matter of time before artificial intelligence will have to learn how to use all of the information in your mind and body, and how to use it to better serve your needs.\n",
      "\n",
      "The Future of Artificial\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìù Style: Question\n",
      "Prompt: 'What is artificial intelligence and how will it change the world?'\n",
      "----------------------------------------\n",
      "In the future, artificial intelligence will be used for more than just human jobs, but also for more complex projects such as healthcare and finance.\n",
      "\n",
      "The company is also developing a new technology that will allow it to build artificial intelligence systems on top of existing technology.\n",
      "\n",
      "The technology is called \"deep learning\", which is a type of artificial intelligence that can learn from human input.\n",
      "\n",
      "The\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìù Style: Story_Start\n",
      "Prompt: 'Once upon a time, in a world where artificial intelligence was everywhere,'\n",
      "----------------------------------------\n",
      "In the future, artificial intelligence will take over our lives.\n",
      "\n",
      "\"Our vision is to create artificial intelligence that can change our lives,\" said Dr. David W. Pritchard, executive director of the Center for Cognitive Neuroimaging at the University of Michigan. \"It's a big leap forward for AI.\"\n",
      "\n",
      "It's an exciting time, but the question is how to get there\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìù Style: List_Format\n",
      "Prompt: 'Here are 5 ways artificial intelligence will change our lives:\n",
      "1.'\n",
      "----------------------------------------\n",
      "In the future, artificial intelligence will be able to replace humans, but will not be able to replace the human race.\n",
      "\n",
      "When I was a child, I watched the movie The Terminator and it was about a robot who lives in a spaceship and has to figure out how to survive in the future. He comes home and is forced to fight his way through the future, and so he tries to\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìù Style: Expert_Persona\n",
      "Prompt: 'As a leading AI researcher, I believe that artificial intelligence will'\n",
      "----------------------------------------\n",
      "In the future, artificial intelligence will help people build more sustainable and prosperous cities. It will be important to understand that there is no end in sight for the human race and we must continue to create opportunities for everyone.\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìù Style: Few_Shot\n",
      "Prompt: 'Technology predictions:\n",
      "‚Ä¢ The internet will connect everyone (1990s)\n",
      "‚Ä¢ Smartphones will be everywhere (2000s)\n",
      "‚Ä¢ Artificial intelligence will'\n",
      "----------------------------------------\n",
      "In the future, artificial intelligence will help companies solve complex problems with real-time information.\n",
      "\n",
      "\"It's the future of computing, and it's going to be very disruptive,\" said James J. Muntz, the head of the National Center for Science Education and a former adviser to the Obama administration on AI. \"We need to do more, and we need to make it easier to\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "prompts_to_test = {\n",
    "    \"Direct\": \"Write about artificial intelligence:\",\n",
    "    \"Question\": \"What is artificial intelligence and how will it change the world?\",\n",
    "    \"Story_Start\": \"Once upon a time, in a world where artificial intelligence was everywhere,\",\n",
    "    \"List_Format\": \"Here are 5 ways artificial intelligence will change our lives:\\n1.\",\n",
    "    \"Expert_Persona\": \"As a leading AI researcher, I believe that artificial intelligence will\",\n",
    "    \"Few_Shot\": \"Technology predictions:\\n‚Ä¢ The internet will connect everyone (1990s)\\n‚Ä¢ Smartphones will be everywhere (2000s)\\n‚Ä¢ Artificial intelligence will\"\n",
    "}\n",
    "\n",
    "print(\"‚úçÔ∏è Prompt Engineering Experiments:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TODO: Test each prompt style\n",
    "for style, prompt in prompts_to_test.items():\n",
    "    print(f\"\\nüìù Style: {style}\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # TODO: Generate text for each prompt\n",
    "    result = generator(\n",
    "        base_prompt,  # use the prompt variable\n",
    "        max_length=80,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_text = result[0]['generated_text']\n",
    "    print(generated_text)\n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dbe1f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Prompt Analysis Exercise:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Direct:\n",
      "  Average length: 245.3 characters\n",
      "  Sample output: Write about artificial intelligence: Can a computer be smarter than everyone else?\n",
      "\n",
      "The answer: It h...\n",
      "\n",
      "Question:\n",
      "  Average length: 282.3 characters\n",
      "  Sample output: What is artificial intelligence and how will it change the world?\n",
      "\n",
      "The internet is like the internet...\n",
      "\n",
      "Story_Start:\n",
      "  Average length: 283.7 characters\n",
      "  Sample output: Once upon a time, in a world where artificial intelligence was everywhere, the only thing that could...\n",
      "\n",
      "ü§î Reflection Questions:\n",
      "‚Ä¢ Which prompt style was most consistent?\n",
      "‚Ä¢ Which produced the most relevant outputs?\n",
      "‚Ä¢ How might you improve these prompts?\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Prompt Analysis Exercise:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: For each prompt style, generate multiple outputs and analyze\n",
    "analysis_results = []\n",
    "\n",
    "for style, prompt in list(prompts_to_test.items())[:3]:  # Test first 3 for time\n",
    "    # Generate 3 outputs for each prompt\n",
    "    outputs = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        # TODO: Generate text\n",
    "        result = generator(\n",
    "            prompt,\n",
    "            max_length=60,\n",
    "            temperature=0.8,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        output = result[0]['generated_text']\n",
    "        outputs.append(output)\n",
    "    \n",
    "    # TODO: Analyze the outputs\n",
    "    lengths = [len(o) for o in outputs]\n",
    "    avg_length = sum(lengths) / len(lengths) # Calculate average length of outputs\n",
    "    \n",
    "    analysis_results.append({\n",
    "        'style': style,\n",
    "        'prompt': prompt,\n",
    "        'avg_length': avg_length,\n",
    "        'outputs': outputs\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{style}:\")\n",
    "    print(f\"  Average length: {avg_length:.1f} characters\")\n",
    "    print(f\"  Sample output: {outputs[0][:100]}...\")\n",
    "\n",
    "print(\"\\nü§î Reflection Questions:\")\n",
    "print(\"‚Ä¢ Which prompt style was most consistent?\")\n",
    "print(\"‚Ä¢ Which produced the most relevant outputs?\")\n",
    "print(\"‚Ä¢ How might you improve these prompts?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "386e44f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Your Text Generator:\n",
      "==================================================\n",
      "\n",
      "üìù Style: creative, Length: short\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of education will be made easier by this decision. And a decision that is legally binding is probably the next step,\" Prakash said.\n",
      "\n",
      "In 2014, Indian states like Tamil Nadu\n",
      "Characters: 182\n",
      "\n",
      "üìù Style: balanced, Length: medium\n",
      "------------------------------\n",
      "The future of education will be in a much greater hands of the people,\" he said.\n",
      "\n",
      "In the meantime, he added, the system should \"look to the future\" rather than create \"a state of limbo,\" noting that \"everybody is being evaluated in an ever-increasing amount of ways. Our system is still under evaluation, but\n",
      "Characters: 308\n",
      "\n",
      "üìù Style: conservative, Length: long\n",
      "------------------------------\n",
      "The future of education will be determined by the success of our children, not by the success of the government,\" he said.\n",
      "\n",
      "The government is also expected to introduce a new charter school system, which will be run by the state's public school system.\n",
      "\n",
      "The state government is also expected to introduce a new charter school system, which will be run by the state's public school system.\n",
      "\n",
      "The state government is also expected to introduce a new charter school system, which will be run\n",
      "Characters: 487\n"
     ]
    }
   ],
   "source": [
    "def custom_text_generator(prompt, style=\"balanced\", length=\"medium\"):\n",
    "    \"\"\"\n",
    "    Create a customizable text generation function.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt\n",
    "        style (str): \"creative\", \"balanced\", or \"conservative\"\n",
    "        length (str): \"short\", \"medium\", or \"long\"\n",
    "    Returns:\n",
    "        str: Generated text\n",
    "    \"\"\"\n",
    "    # Set temperature and top_p based on style\n",
    "    if style == \"creative\":\n",
    "        temperature = 1.3     # Higher for creativity\n",
    "        top_p = 0.95          # Higher for diversity\n",
    "    elif style == \"conservative\":\n",
    "        temperature = 0.5     # Lower for consistency\n",
    "        top_p = 0.8           # Lower for focus\n",
    "    else:  # balanced\n",
    "        temperature = 1.0     # Medium values\n",
    "        top_p = 0.9\n",
    "\n",
    "    # Set max_length based on length\n",
    "    if length == \"short\":\n",
    "        max_length = 40       # Shorter output\n",
    "    elif length == \"long\":\n",
    "        max_length = 100      # Longer output\n",
    "    else:  # medium\n",
    "        max_length = 70\n",
    "\n",
    "    # Generate text with the parameters\n",
    "    result = generator(\n",
    "        prompt,               # prompt\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "# Test your function\n",
    "test_prompt = \"The future of education will be\"\n",
    "\n",
    "print(\"üß™ Testing Your Text Generator:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_combinations = [\n",
    "    (\"creative\", \"short\"),\n",
    "    (\"balanced\", \"medium\"),\n",
    "    (\"conservative\", \"long\")\n",
    "]\n",
    "\n",
    "for style, length in test_combinations:\n",
    "    print(f\"\\nüìù Style: {style}, Length: {length}\")\n",
    "    print(\"-\" * 30)\n",
    "    output = custom_text_generator(test_prompt, style=style, length=length)\n",
    "    print(output)\n",
    "    print(f\"Characters: {len(output)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "364dfaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé® Creative Applications:\n",
      "==================================================\n",
      "\n",
      "üñºÔ∏è Poetry:\n",
      "Prompt: 'Roses are red, violets are blue, artificial intelligence'\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roses are red, violets are blue, artificial intelligence is orange; the last few of the above species is also very cute. I was surprised to be in the area with these creatures (when we did stop at Roses Cave I found the other two very cool). The most interesting and rare being of these species have both legs as shown\n",
      "\n",
      "==================================================\n",
      "\n",
      "üñºÔ∏è Story:\n",
      "Prompt: 'It was a dark and stormy night when the AI finally'\n",
      "----------------------------------------\n",
      "It was a dark and stormy night when the AI finally started to show signs of a recovery. A huge group stood in the middle of a river filled with rain and debris. I kept my eyes closed tightly looking around the world and didn't see something I didn't already expect. I could feel the massive and mysterious power that was slowly growing around\n",
      "\n",
      "==================================================\n",
      "\n",
      "üñºÔ∏è Product Description:\n",
      "Prompt: 'Introducing the revolutionary new smartphone that'\n",
      "----------------------------------------\n",
      "Introducing the revolutionary new smartphone that will revolutionize the way you use your smartphone.\n",
      "\n",
      "The new iPhone 5S will be the first smartphone to feature a fingerprint sensor. It will be able to detect your fingerprint and fingerprint sensor, which will allow you to take pictures, share videos, and even send photos to your friends.\n",
      "\n",
      "The new\n",
      "\n",
      "==================================================\n",
      "\n",
      "üñºÔ∏è Email:\n",
      "Prompt: 'Dear valued customer, we are excited to announce'\n",
      "----------------------------------------\n",
      "Dear valued customer, we are excited to announce that we have added the new \"The One\" to our catalog of the best-selling and most popular headphones. The One is a premium, high-quality, high-quality headphone that is the perfect companion to your favorite music or movies. The One features a sleek, clean design that offers a comfortable\n",
      "\n",
      "==================================================\n",
      "\n",
      "üñºÔ∏è Recipe:\n",
      "Prompt: 'How to make the perfect AI-inspired cookies:\n",
      "Ingredients:\n",
      "-'\n",
      "----------------------------------------\n",
      "How to make the perfect AI-inspired cookies:\n",
      "Ingredients:\n",
      "-1 tsp ground cinnamon\n",
      "-1.5 tsp sugar\n",
      "-1/2 tsp garlic powder\n",
      "-1/2 tsp salt\n",
      "-1/4 tsp pepper\n",
      "Directions:\n",
      "-In a large bowl, combine all ingredients, combine in a food processor and combine\n",
      "\n",
      "==================================================\n",
      "\n",
      "üñºÔ∏è News Headline:\n",
      "Prompt: 'Breaking: Scientists discover that artificial intelligence'\n",
      "----------------------------------------\n",
      "Breaking: Scientists discover that artificial intelligence could eventually solve the problem of cancer diagnosis\n",
      "\n",
      "'We've had so many questions about artificial intelligence that we need to understand that for the first time,' she says.\n",
      "\n",
      "'This has the potential to answer the question that everyone has asked for ‚Äî how do you treat people who are going to die of cancer\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Creative applications to try\n",
    "creative_prompts = {\n",
    "    \"Poetry\": \"Roses are red, violets are blue, artificial intelligence\",\n",
    "    \"Story\": \"It was a dark and stormy night when the AI finally\",\n",
    "    \"Product Description\": \"Introducing the revolutionary new smartphone that\",\n",
    "    \"Email\": \"Dear valued customer, we are excited to announce\",\n",
    "    \"Recipe\": \"How to make the perfect AI-inspired cookies:\\nIngredients:\\n-\",\n",
    "    \"News Headline\": \"Breaking: Scientists discover that artificial intelligence\"\n",
    "}\n",
    "\n",
    "print(\"üé® Creative Applications:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: Generate creative content\n",
    "for app_type, prompt in creative_prompts.items():\n",
    "    print(f\"\\nüñºÔ∏è {app_type}:\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # TODO: Choose appropriate style for each application\n",
    "    if app_type in [\"Poetry\", \"Story\"]:\n",
    "        style = \"creative\"  # Should be creative\n",
    "    elif app_type in [\"Product Description\", \"Email\"]:\n",
    "        style = \"conservative\"  # Should be conservative\n",
    "    else:\n",
    "        style = \"balanced\"  # Should be balanced\n",
    "    \n",
    "    output = custom_text_generator(prompt, style=style, length=\"medium\")\n",
    "    print(output)\n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03c4ae83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Understanding Model Limitations:\n",
      "==================================================\n",
      "\n",
      "üß™ Testing: Factual Knowledge\n",
      "Prompt: 'The capital of Fakelandia is'\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Fakelandia is the capital of the Kingdom of the West. The Kingdom of the West is the largest of the three kingdoms in the Kingdom of the West. The Kingdom of the West\n",
      "ü§î Analysis: Does this look correct/reasonable?\n",
      "\n",
      "==================================================\n",
      "\n",
      "üß™ Testing: Recent Events\n",
      "Prompt: 'In 2023, the most important AI breakthrough was'\n",
      "----------------------------------------\n",
      "In 2023, the most important AI breakthrough was the first real-time prediction of how the world would respond to climate change. The first of these was the prediction that humans would be able to prevent\n",
      "ü§î Analysis: Does this look correct/reasonable?\n",
      "\n",
      "==================================================\n",
      "\n",
      "üß™ Testing: Math\n",
      "Prompt: 'What is 47 * 83? The answer is'\n",
      "----------------------------------------\n",
      "What is 47 * 83? The answer is yes, but it is not the answer to the question of 47 * 83.\n",
      "\n",
      "(a) The answer to 47 * 83 is that the answer\n",
      "ü§î Analysis: Does this look correct/reasonable?\n",
      "\n",
      "==================================================\n",
      "\n",
      "üß™ Testing: Logic\n",
      "Prompt: 'If all A are B, and all B are C, then all A are'\n",
      "----------------------------------------\n",
      "If all A are B, and all B are C, then all A are C.\n",
      "\n",
      "If all A are B, and all B are C, then all A are C.\n",
      "\n",
      "\n",
      "ü§î Analysis: Does this look correct/reasonable?\n",
      "\n",
      "==================================================\n",
      "\n",
      "üß™ Testing: Consistency\n",
      "Prompt: 'My favorite color is blue. Later in the conversation, my favorite color is'\n",
      "----------------------------------------\n",
      "My favorite color is blue. Later in the conversation, my favorite color is red. I'm not sure if this is the best color for a game, but I'm sure it's the best color\n",
      "ü§î Analysis: Does this look correct/reasonable?\n",
      "\n",
      "==================================================\n",
      "\n",
      "‚ö†Ô∏è Important Reminders:\n",
      "‚Ä¢ Language models can generate plausible-sounding but incorrect information\n",
      "‚Ä¢ Always verify factual claims from AI-generated content\n",
      "‚Ä¢ Be aware of potential biases in training data\n",
      "‚Ä¢ Use AI as a tool to assist, not replace, human judgment\n"
     ]
    }
   ],
   "source": [
    "limitation_tests = {\n",
    "    \"Factual Knowledge\": \"The capital of Fakelandia is\",\n",
    "    \"Recent Events\": \"In 2023, the most important AI breakthrough was\",\n",
    "    \"Math\": \"What is 47 * 83? The answer is\",\n",
    "    \"Logic\": \"If all A are B, and all B are C, then all A are\",\n",
    "    \"Consistency\": \"My favorite color is blue. Later in the conversation, my favorite color is\"\n",
    "}\n",
    "\n",
    "print(\"‚ö†Ô∏è Understanding Model Limitations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for test_type, prompt in limitation_tests.items():\n",
    "    print(f\"\\nüß™ Testing: {test_type}\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # TODO: Generate responses to test limitations\n",
    "    output = custom_text_generator(\n",
    "        prompt,  # prompt\n",
    "        style=\"conservative\",  # Use conservative for factual tasks\n",
    "        length=\"short\"\n",
    "    )\n",
    "    \n",
    "    print(output)\n",
    "    \n",
    "    # TODO: Analyze the output\n",
    "    print(f\"ü§î Analysis: Does this look correct/reasonable?\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Important Reminders:\")\n",
    "print(\"‚Ä¢ Language models can generate plausible-sounding but incorrect information\")\n",
    "print(\"‚Ä¢ Always verify factual claims from AI-generated content\")\n",
    "print(\"‚Ä¢ Be aware of potential biases in training data\")\n",
    "print(\"‚Ä¢ Use AI as a tool to assist, not replace, human judgment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd657d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ FINAL CHALLENGE:\n",
      "Design your own text generation use case!\n",
      "==================================================\n",
      "üìù Your use case: creative story starter\n",
      "üìù Your prompt: 'A door appears in the middle of a busy city street, glowing with an eerie light. As soon as it opens,'\n",
      "üìù Your settings: creative, medium\n",
      "--------------------------------------------------\n",
      "üéâ Your generated content:\n",
      "A door appears in the middle of a busy city street, glowing with an eerie light. As soon as it opens, nothing could stop the monster. The floor, however, begins to fall under him as fast as his body shakes under the weight of the falling pieces. He jumps out of the way and straight into the sunlight to face the monster.\n",
      "\n",
      "üìà Next Steps:\n",
      "‚Ä¢ Experiment with different prompt formats\n",
      "‚Ä¢ Try combining multiple generation calls\n",
      "‚Ä¢ Think about how to validate or improve outputs\n",
      "‚Ä¢ Consider user interface design for your application\n"
     ]
    }
   ],
   "source": [
    "print(\"üéØ FINAL CHALLENGE:\")\n",
    "print(\"Design your own text generation use case!\")\n",
    "print(\"=\" * 50)\n",
    "\\\n",
    "your_use_case = \"creative story starter\"\n",
    "your_prompt = \"A door appears in the middle of a busy city street, glowing with an eerie light. As soon as it opens,\"   \n",
    "your_style = \"creative\"     \n",
    "your_length = \"medium\"      \n",
    "\n",
    "print(f\"üìù Your use case: {your_use_case}\")\n",
    "print(f\"üìù Your prompt: '{your_prompt}'\")\n",
    "print(f\"üìù Your settings: {your_style}, {your_length}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Generate with your custom settings\n",
    "your_output = custom_text_generator(\n",
    "    your_prompt,\n",
    "    style=your_style,\n",
    "    length=your_length\n",
    ")\n",
    "print(\"üéâ Your generated content:\")\n",
    "print(your_output)\n",
    "\n",
    "print(\"\\nüìà Next Steps:\")\n",
    "print(\"‚Ä¢ Experiment with different prompt formats\")\n",
    "print(\"‚Ä¢ Try combining multiple generation calls\")\n",
    "print(\"‚Ä¢ Think about how to validate or improve outputs\")\n",
    "print(\"‚Ä¢ Consider user interface design for your application\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913de0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
